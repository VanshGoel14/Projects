{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f0fac7",
   "metadata": {},
   "source": [
    "# Data Engineering Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a46f4f",
   "metadata": {},
   "source": [
    "## YouTube Trends Analysis  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6087b03",
   "metadata": {},
   "source": [
    "## Tools used: AWS - S3, Athena, Lambda, Glue, QuickSight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf27af",
   "metadata": {},
   "source": [
    "### Part -1\n",
    "### Downloaded youtube dataset from Kaggle https://www.kaggle.com/datasets/datasnaek/youtube-new \n",
    "### The downloaded data is in JSON and CSV formats\n",
    "### Loaded the data onto S3 Bucket using CLI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a008a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To copy all JSON Reference data to same location:\n",
    "aws s3 cp . s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics_reference_data/ --recursive --exclude \"*\" --include \"*.json\"\n",
    "\n",
    "# To copy all data files to its own location, following Hive-style patterns:\n",
    "aws s3 cp CAvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=ca/\n",
    "aws s3 cp DEvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=de/\n",
    "aws s3 cp FRvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=fr/\n",
    "aws s3 cp GBvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=gb/\n",
    "aws s3 cp INvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=in/\n",
    "aws s3 cp JPvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=jp/\n",
    "aws s3 cp KRvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=kr/\n",
    "aws s3 cp MXvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=mx/\n",
    "aws s3 cp RUvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=ru/\n",
    "aws s3 cp USvideos.csv s3://de-on-youtube-raw-useast1-dev/youtube/raw_statistics/region=us/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f74fd0",
   "metadata": {},
   "source": [
    "### Created a Crawler on AWS Glue to go through the data and create a table\n",
    "### Ran ad hoc analysis on the created table using AWS Athena\n",
    "### Ran into some errors as it could not process the data as a JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c68b86",
   "metadata": {},
   "source": [
    "### Part -2\n",
    "### Pre-Processing and cleaning data stage\n",
    "### Created a new bucket to store the processed data\n",
    "### Created a function using AWS Lambda to convert the JSON file into Parquet format\n",
    "### Ran into some errors during this process which were swiftly resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "# Temporary hard-coded AWS Settings; i.e. to be set as OS variable in Lambda\n",
    "os_input_s3_cleansed_layer = os.environ['s3_cleansed_layer']\n",
    "os_input_glue_catalog_db_name = os.environ['glue_catalog_db_name']\n",
    "os_input_glue_catalog_table_name = os.environ['glue_catalog_table_name']\n",
    "os_input_write_data_operation = os.environ['write_data_operation']\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Get the object from the event and show its content type\n",
    "    bucket = event['Records'][0]['s3']['bucket']['name']\n",
    "    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')\n",
    "    try:\n",
    "\n",
    "        # Creating DF from content\n",
    "        df_raw = wr.s3.read_json('s3://{}/{}'.format(bucket, key))\n",
    "\n",
    "        # Extract required columns:\n",
    "        df_step_1 = pd.json_normalize(df_raw['items'])\n",
    "\n",
    "        # Write to S3\n",
    "        wr_response = wr.s3.to_parquet(\n",
    "            df=df_step_1,\n",
    "            path=os_input_s3_cleansed_layer,\n",
    "            dataset=True,\n",
    "            database=os_input_glue_catalog_db_name,\n",
    "            table=os_input_glue_catalog_table_name,\n",
    "            mode=os_input_write_data_operation\n",
    "        )\n",
    "\n",
    "        return wr_response\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a669d5c",
   "metadata": {},
   "source": [
    "### More Pre-procesing as the queries I was running on AWS Athena were too slow and required more opitimization\n",
    "### Created a Job on AWS Glue that converts all CSV files into Parquet format \n",
    "### Made some modifications to the auto-generated job code shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a49bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "############################### Added by Carlos ###############################\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "## @type: DataSource\n",
    "## @args: [database = \"db_youtube_raw\", table_name = \"raw_statistics\", transformation_ctx = \"datasource0\"]\n",
    "## @return: datasource0\n",
    "## @inputs: []\n",
    "\n",
    "predicate_pushdown = \"region in ('ca','gb','us')\"\n",
    "\n",
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(database = \"db_youtube_raw\", table_name = \"raw_statistics\", transformation_ctx = \"datasource0\", push_down_predicate = predicate_pushdown)\n",
    "\n",
    "## @type: ApplyMapping\n",
    "## @args: [mapping = [(\"video_id\", \"string\", \"video_id\", \"string\"), (\"trending_date\", \"string\", \"trending_date\", \"string\"), (\"title\", \"string\", \"title\", \"string\"), (\"channel_title\", \"string\", \"channel_title\", \"string\"), (\"category_id\", \"long\", \"category_id\", \"long\"), (\"publish_time\", \"string\", \"publish_time\", \"string\"), (\"tags\", \"string\", \"tags\", \"string\"), (\"views\", \"long\", \"views\", \"long\"), (\"likes\", \"long\", \"likes\", \"long\"), (\"dislikes\", \"long\", \"dislikes\", \"long\"), (\"comment_count\", \"long\", \"comment_count\", \"long\"), (\"thumbnail_link\", \"string\", \"thumbnail_link\", \"string\"), (\"comments_disabled\", \"boolean\", \"comments_disabled\", \"boolean\"), (\"ratings_disabled\", \"boolean\", \"ratings_disabled\", \"boolean\"), (\"video_error_or_removed\", \"boolean\", \"video_error_or_removed\", \"boolean\"), (\"description\", \"string\", \"description\", \"string\"), (\"region\", \"string\", \"region\", \"string\")], transformation_ctx = \"applymapping1\"]\n",
    "## @return: applymapping1\n",
    "## @inputs: [frame = datasource0]\n",
    "applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [(\"video_id\", \"string\", \"video_id\", \"string\"), (\"trending_date\", \"string\", \"trending_date\", \"string\"), (\"title\", \"string\", \"title\", \"string\"), (\"channel_title\", \"string\", \"channel_title\", \"string\"), (\"category_id\", \"long\", \"category_id\", \"long\"), (\"publish_time\", \"string\", \"publish_time\", \"string\"), (\"tags\", \"string\", \"tags\", \"string\"), (\"views\", \"long\", \"views\", \"long\"), (\"likes\", \"long\", \"likes\", \"long\"), (\"dislikes\", \"long\", \"dislikes\", \"long\"), (\"comment_count\", \"long\", \"comment_count\", \"long\"), (\"thumbnail_link\", \"string\", \"thumbnail_link\", \"string\"), (\"comments_disabled\", \"boolean\", \"comments_disabled\", \"boolean\"), (\"ratings_disabled\", \"boolean\", \"ratings_disabled\", \"boolean\"), (\"video_error_or_removed\", \"boolean\", \"video_error_or_removed\", \"boolean\"), (\"description\", \"string\", \"description\", \"string\"), (\"region\", \"string\", \"region\", \"string\")], transformation_ctx = \"applymapping1\")\n",
    "## @type: ResolveChoice\n",
    "## @args: [choice = \"make_struct\", transformation_ctx = \"resolvechoice2\"]\n",
    "## @return: resolvechoice2\n",
    "## @inputs: [frame = applymapping1]\n",
    "resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = \"make_struct\", transformation_ctx = \"resolvechoice2\")\n",
    "## @type: DropNullFields\n",
    "## @args: [transformation_ctx = \"dropnullfields3\"]\n",
    "## @return: dropnullfields3\n",
    "## @inputs: [frame = resolvechoice2]\n",
    "dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = \"dropnullfields3\")\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", connection_options = {\"path\": \"s3://bigdata-on-youtube-cleansed-euwest1-14317621-dev/youtube/raw_statistics/\"}, format = \"parquet\", transformation_ctx = \"datasink4\"]\n",
    "## @return: datasink4\n",
    "## @inputs: [frame = dropnullfields3]\n",
    "\n",
    "\n",
    "datasink1 = dropnullfields3.toDF().coalesce(1)\n",
    "df_final_output = DynamicFrame.fromDF(datasink1, glueContext, \"df_final_output\")\n",
    "datasink4 = glueContext.write_dynamic_frame.from_options(frame = df_final_output, connection_type = \"s3\", connection_options = {\"path\": \"s3://bigdata-on-youtube-cleansed-euwest1-14317621-dev/youtube/raw_statistics/\", \"partitionKeys\": [\"region\"]}, format = \"parquet\", transformation_ctx = \"datasink4\")\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17fc56",
   "metadata": {},
   "source": [
    "### Part- 3\n",
    "### Ran a Crawler through the converted data to run some ad-hoc analysis using Glue and Athena\n",
    "### Created a trigger in AWS Lambda that converts JSON files Into Parquet files\n",
    "### The trigger does the conversion whenever a .JSON file is uploaded into the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34b6e0",
   "metadata": {},
   "source": [
    "### Created a new database to store all the converted Parquet files \n",
    "### Created a job On AWS Glue that does joins based on category and region and stores the files into the new Database\n",
    "### Ran analysis on the database using AWS QuickSight "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
